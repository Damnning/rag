{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_YWGcIOszJn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class Metric(Enum):\n",
    "    '''\n",
    "    Vector distance metrics enum\n",
    "    '''\n",
    "    COS = \"_cos\"\n",
    "    SCALAR = \"_scalar\"\n",
    "\n",
    "    def __call__(self, a, b):\n",
    "        method = getattr(self, self.value)\n",
    "        return method(a, b)\n",
    "\n",
    "    def is_reversed(self):\n",
    "        return self == Metric.COS or self == Metric.SCALAR\n",
    "\n",
    "    def _cos(self, a, b):\n",
    "        return torch.dot(a, b)/(torch.norm(a.to(torch.float32))*torch.norm(b.to(torch.float32)))\n",
    "\n",
    "    def _scalar(self, a, b):\n",
    "        return torch.dot(a.to(torch.float32), b.to(torch.float32))\n",
    "\n",
    "class VectorStore:\n",
    "    '''\n",
    "    Class for working with vectors, recieving nearest neighbors\n",
    "    '''\n",
    "    def __init__(self, embedings, sort_metric: Metric):\n",
    "        self.embedings = embedings\n",
    "        self.sort_metric = sort_metric\n",
    "\n",
    "    def get_k_nearest(self, query_embeding, k=None):\n",
    "        sorted_items = sorted(\n",
    "            self.embedings.items(),\n",
    "            key=lambda x: self.sort_metric(query_embeding, torch.tensor(x[1])),\n",
    "            reverse=self.sort_metric.is_reversed()\n",
    "        )\n",
    "        return list(dict(sorted_items[:k]).keys())\n",
    "\n",
    "\n",
    "class DocumentRetriever:\n",
    "    '''\n",
    "    Class for working with files\n",
    "    '''\n",
    "\n",
    "    vectors = 'text_vectors.json'\n",
    "    articles = 'articles'\n",
    "\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.articles_dir = os.path.join(self.root, self.articles)\n",
    "        self.vectors_path = os.path.join(self.root, self.vectors)\n",
    "\n",
    "    def get_articles_by_filenames(self, filenames: list[str]) -> list[str]:\n",
    "        articles = []\n",
    "        for f in filenames:\n",
    "            articles.append(self._get_article_text(f))\n",
    "        return articles\n",
    "\n",
    "    def get_all_articles(self) -> dict:\n",
    "        articles = {}\n",
    "        for f in tqdm(os.listdir(self.articles_dir)):\n",
    "            articles[f] = self._get_article_text(f)\n",
    "        return articles\n",
    "\n",
    "    def _get_article_text(self, filename: str) -> str:\n",
    "        with open(os.path.join(self.articles_dir, filename)) as file:\n",
    "            raw = file.read()\n",
    "            # splt = re.split('{{NAME}}|{{/NAME}}|{{DESC}}|{{/DESC}}', re.sub('\\n(?:[ \\t]*\\n)+', '', raw))\n",
    "            return raw #splt[1] + splt[2] + splt[3]\n",
    "\n",
    "    def is_ready(self) -> bool:\n",
    "        if(os.path.exists(os.path.join(self.vectors_path))):\n",
    "            filenames = os.listdir(self.articles_dir)\n",
    "            embedings = self.get_articles_embedings_json()\n",
    "            for f in filenames:\n",
    "                if(f not in embedings):\n",
    "                    return False\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_articles_embedings_json(self) -> dict:\n",
    "        try:\n",
    "            with open(self.vectors_path) as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            return {}\n",
    "\n",
    "    def save_articles_embedings_json(self, embedings: dict) -> None:\n",
    "        with open(self.vectors_path, 'w') as f:\n",
    "            json.dump(embedings, f)\n",
    "\n",
    "class PromptGenerator:\n",
    "    '''\n",
    "    Class for fromatting prompts for LLM\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def build_prompt(query, articles) -> str:\n",
    "        prompt = f'Найди ответ на этот запрос: {query} \\nОсновываясь на статьях далее, не следует в ответ включать информацию, которая в них не содержится: \\n{articles}'\n",
    "        return prompt\n",
    "\n",
    "class DataProcessor:\n",
    "    '''\n",
    "    Class for turning texts into embeddings\n",
    "    '''\n",
    "    _instance = None\n",
    "\n",
    "    def __new__(cls, model_name: str, document_retrivier: DocumentRetriever):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(DataProcessor, cls).__new__(cls)\n",
    "            cls._instance.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            cls._instance.model = AutoModel.from_pretrained(model_name)\n",
    "            cls._instance.document_retrivier = document_retrivier\n",
    "            if not cls._instance.document_retrivier.is_ready():\n",
    "                cls._instance._prepare_data()\n",
    "        return cls._instance\n",
    "\n",
    "    def get_articles_embedings_json(self):\n",
    "        return self.document_retrivier.get_articles_embedings_json()\n",
    "\n",
    "    def _pool(self, hidden_state, mask, pooling_method=\"cls\"):\n",
    "        if pooling_method == \"mean\":\n",
    "            s = torch.sum(hidden_state * mask.unsqueeze(-1).float(), dim=1)\n",
    "            d = mask.sum(axis=1, keepdim=True).float()\n",
    "            return s / d\n",
    "        elif pooling_method == \"cls\":\n",
    "            return hidden_state[:, 0]\n",
    "\n",
    "    def get_embeding(self, text: str) -> torch.Tensor:\n",
    "        tokenized_input = self.tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokenized_input)\n",
    "        embedding = self._pool(\n",
    "            outputs.last_hidden_state,\n",
    "            tokenized_input[\"attention_mask\"],\n",
    "            pooling_method=\"mean\"\n",
    "        )\n",
    "\n",
    "        embedding = F.normalize(embedding, p=2, dim=1)\n",
    "        return embedding.flatten()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        embeddings = {}\n",
    "        articles = self.document_retrivier.get_all_articles()\n",
    "        print(articles)\n",
    "        for k, v in tqdm(articles.items()):\n",
    "            embeddings[k] = self.get_embeding(v).tolist()\n",
    "        self.document_retrivier.save_articles_embedings_json(embeddings)\n",
    "\n",
    "class LLMWrapper:\n",
    "    '''\n",
    "    Class for interacting with LLM\n",
    "    '''\n",
    "    _instance = None\n",
    "\n",
    "    def __new__(cls, model_name: str, api_key: str):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(LLMWrapper, cls).__new__(cls)\n",
    "            cls._instance.base_url = \"https://openrouter.ai/api/v1\"\n",
    "            cls._instance.api_key = api_key\n",
    "            cls._instance.model_name = model_name\n",
    "            cls._instance.client = OpenAI(\n",
    "                base_url=\"https://openrouter.ai/api/v1\",\n",
    "                api_key=cls._instance.api_key\n",
    "            )\n",
    "        return cls._instance\n",
    "\n",
    "    def generate_response(self, prompt: str) -> str:\n",
    "        try:\n",
    "            completion = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            if completion and completion.choices:\n",
    "                return completion.choices[0].message.content\n",
    "            else:\n",
    "                return \"Error: No response from the API\"\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "class Config:\n",
    "    '''\n",
    "    Class for configurating RAG\n",
    "\n",
    "    Attributes:\n",
    "        root (str): path to directory containing articles in folder 'articles'\n",
    "        embedding_model (str): link to huggingface model for generating embeddings\n",
    "        llm_model (str): link to openrouter model for generating answers\n",
    "        api_key (str): openrouter api key\n",
    "        k (int): number of nearest neighbors to use for generating answer\n",
    "    '''\n",
    "    def __init__(self, root: str, embedding_model: str, llm_model: str, api_key: str, k: str):\n",
    "        self.root = root\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm_model = llm_model\n",
    "        self.api_key = api_key\n",
    "        self.k = k\n",
    "\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.document_retriever = DocumentRetriever(self.config.root)\n",
    "        self.data_processor = DataProcessor(self.config.embedding_model, self.document_retriever)\n",
    "        self.vector_store = VectorStore(self.data_processor.get_articles_embedings_json(), Metric.COS)\n",
    "        self.llm = LLMWrapper(self.config.llm_model, self.config.api_key)\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        files = self.vector_store.get_k_nearest(self.data_processor.get_embeding(query), k=self.config.k)\n",
    "        articles = self.document_retriever.get_articles_by_filenames(files)\n",
    "        prompt = PromptGenerator.build_prompt(query, articles)\n",
    "        out = self.llm.generate_response(prompt)\n",
    "        return out"
   ],
   "metadata": {
    "id": "dodIW9FQ01YT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "config = Config(root='/articles',\n",
    "                embedding_model='ai-forever/ru-en-RoSBERTa',\n",
    "                llm_model='deepseek/deepseek-chat-v3-0324:free',\n",
    "                api_key='',\n",
    "                k=20)"
   ],
   "metadata": {
    "id": "4OoOpbEwzcQv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pipe = RAGPipeline(config)"
   ],
   "metadata": {
    "id": "yIoUYUYMWiRd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(pipe.run('Что можно подарить Ване Транькову на день рождения'))"
   ],
   "metadata": {
    "id": "Ae1xTKUw2rHb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "92675400-f176-45c8-e98e-e19bdf71e361"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "На основе предоставленных фрагментов переписки можно выделить несколько идей для подарка Ване Транькову на день рождения:\n",
      "\n",
      "1. **Книга по алгоритмам**  \n",
      "   Упоминание книги *«Грокаем алгоритмы»* (1 место среди часто краденых) намекает на его интерес к программированию. Такой подарок будет полезным и актуальным.  \n",
      "\n",
      "2. **Тематический мерч или шутливый подарок**  \n",
      "   - Футболка или кружка с надписью про «24 — всего лишь цифра» (отсылка к фразе Александра Правдина).  \n",
      "   - Сувенир, связанный с «Тарасовым» (например, табличка «Не бояться Тарасова»), так как в переписке часто обсуждаются стрессовые моменты обучения.  \n",
      "\n",
      "3. **Гаджеты или аксессуары для работы**  \n",
      "   - USB-хаб или внешний жесткий диск, учитывая его активную работу с проектами и данными.  \n",
      "\n",
      "4. **Настольная игра**  \n",
      "   Например, *«Бирмингем»* (упоминается в контексте вечеринки) или другую стратегическую игру, поскольку ребята любят проводить время вместе.  \n",
      "\n",
      "5. **Шуточный сертификат**  \n",
      "   Например, «Сертификат на спасение от запряга» (отсылка к его словам: *«Вы так меня с запрягом спасли»*), оформленный в стиле внутренних шуток чата.  \n",
      "\n",
      "Если выбрать что-то, связанное с общим контекстом учебы и программирования, но с долей юмора, — это будет идеально. Избегайте личных тем, не упомянутых в переписке (например, хобби вне IT).  \n",
      "\n",
      "**Оптимальный вариант**: книга *«Грокаем алгоритмы»* + шуточный мерч (например, кружка с надписью *«Не бояться Тарасова»*). Это сочетает полезность и внутренние мемы компании.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "P_X0JvSNWYP1"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
